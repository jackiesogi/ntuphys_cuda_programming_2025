># Introduction to CUDA Parallel Programming HW4
```
Author: NTUST M11315133 陳首吉
Date: June 5, 2025
```

## Implementation

Each CUDA block computes a partial dot product by having threads accumulate products over the vector in a strided fashion, then reduces these partial sums within shared memory.


```cpp
/* Device reduction kernel: each block computes a partial dot‐product */
__global__ void compute(const float* A, const float* B, float* result, int M) {
    extern __shared__ float cache[];
    int global_index = blockDim.x * blockIdx.x + threadIdx.x;
    int local_index  = threadIdx.x;
    int stride       = blockDim.x * gridDim.x;

    float temp = 0.0f;
    /* Strided load+multiply loop */
    while (global_index < M) {
        temp += A[global_index] * B[global_index];
        global_index += stride;
    }

    cache[local_index] = temp;
    __syncthreads();

    /* Binary reduction in shared memory */
    for (int offset = blockDim.x / 2; offset > 0; offset >>= 1) {
        if (local_index < offset) {
            cache[local_index] += cache[local_index + offset];
        }
        __syncthreads();
    }

    /* Write block result */
    if (local_index == 0) {
        result[blockIdx.x] = cache[0];
    }
}
```

## Performance Analysis

| Threads Per Block | Blocks Per Grid | Host to Device Time (ms) | Kernel Time (ms) | Device to Host Time (ms) | GPU Time (ms) | CPU Time (ms) | Speedup With Data Transfer | Speedup Without Data Transfer | GPU GFLOPS (kernel) | CPU GFLOPS | Norm(cpu_result - gpu_result) |
|-------------------|-----------------|--------------------------|------------------|--------------------------|---------------|---------------|----------------------------|------------------------------|---------------------|------------|-------------------------------|
| 1                 | 32              | 13.78                    | 52.35            | 0.01                     | 66.16         | 37.75         | 0.57x                      | 0.72x                        | 1564.76             | 2170.30    | 845.30                        |
| 4                 | 32              | 13.73                    | 13.63            | 0.02                     | 27.39         | 38.95         | 1.42x                      | 2.86x                        | 6008.68             | 2103.05    | 64.75                         |
| 16                | 32              | 13.87                    | 3.91             | 0.02                     | 17.81         | 38.63         | 2.17x                      | 9.86x                        | 20928.54            | 2120.69    | 4.06                          |
| 64                | 32              | 13.89                    | 1.31             | 0.02                     | 15.23         | 38.69         | 2.54x                      | 29.50x                       | 62600.87            | 2117.07    | 0.40                          |
| 128               | 32              | 13.78                    | 1.15             | 0.02                     | 14.96         | 38.78         | 2.59x                      | 33.65x                       | 71198.13            | 2112.48    | 0.11                          |
| 512               | 32              | 13.89                    | 1.16             | 0.02                     | 15.07         | 39.31         | 2.61x                      | 33.87x                       | 70757.32            | 2083.68    | 0.04                          |
| 1024              | 32              | 13.99                    | 1.13             | 0.02                     | 15.15         | 38.95         | 2.57x                      | 34.37x                       | 72406.38            | 2103.16    | 0.13                          |

- Increasing threads per block reduces kernel time drastically.
- Speedup without data transfer improves significantly with higher threads per block, reaching over 30x.
- GPU GFLOPS scale with increased parallelism, peaking around 72,000 GFLOPS.
- The norm of the difference between CPU and GPU results decreases with larger threads per block, indicating improved accuracy.
- Data transfer time remains relatively constant, limiting overall speedup when included.

