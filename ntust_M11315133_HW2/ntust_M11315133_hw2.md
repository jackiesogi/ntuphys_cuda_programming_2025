># Introduction to CUDA Parallel Programming HW2
```
Author: NTUST M11315133 陳首吉
Date: June 4, 2025
```

## Implementation

The core of the GPU implementation is the `matrixTrace` CUDA kernel. This kernel utilizes parallel reduction within thread blocks using shared memory (`__shared__ float sharedCache[]`) to sum local contributions. A grid-stride loop is employed to ensure all diagonal elements are processed, even if the total number of threads is less than the diagonal size. The kernel requires the number of threads per block to be a power of 2 for the reduction logic. Each block's partial sum is written to a global output array. The final result is obtained by summing these partial block sums on the host CPU.

The host code manages memory allocation on both the host (using pinned memory via `cudaMallocHost` for potentially faster transfers) and the device (`cudaMalloc`). It initializes the diagonal elements using `randomInit`. CUDA events (`cudaEvent_t`) are used to accurately time the Host-to-Device transfer, kernel execution, and Device-to-Host transfer. The sequential CPU baseline calculation and the final merge of partial results on the host are timed using `clock_gettime`. Performance metrics like GPU time (including and excluding transfers), CPU time, speedup, and GFLOPS are computed and printed. An accuracy check verifies the correctness of the GPU result against the CPU result.

We tested the implementation on two different matrix sizes: a matrix of 6400 × 6400 (meaning a diagonal of length 6400). Performance was measured using end-to-end GPU runtime (including transfers), kernel-only runtime, speedup over CPU, and GFLOPS. We performed parameter sweeps by varying the number of threads per block and the number of blocks per grid to find the optimal configuration.

## Results Tables

### Table 1 – Matrix with *Fixed* Total Threads (~8192)

| Threads/Block | Blocks/Grid | Total Threads | GPU Time (ms) | CPU Time (ms) | GPU GFLOPS | CPU GFLOPS |
| :-----------: | :---------: | :-----------: | :-----------: | :-----------: | :--------: | :--------: |
| 1024          | 8           | 8192          | 0.1231        | 0.0050        | 0.0703     | 1.270      |
| 512           | 16          | 8192          | 0.1375        | 0.0051        | 0.0628     | 1.241      |
| **256**       | **32**      | **8192**      | **0.1285**    | **0.0053**    | **0.0746** | **1.204**  |
| **128**       | **64**      | **8192**      | **0.1190**    | **0.0050**    | **0.0769** | **1.277**  |
| 64            | 128         | 8192          | 0.1320        | 0.0051        | 0.0720     | 1.252      |
| 32            | 256         | 8192          | 0.1180        | 0.0051        | 0.0739     | 1.250      |
| 16            | 512         | 8192          | 0.1238        | 0.0051        | 0.0716     | 1.261      |
| 8             | 1024        | 8192          | 0.1415        | 0.0051        | 0.0619     | 1.266      |
| 4             | 2048        | 8192          | 0.1545        | 0.0054        | 0.0535     | 1.181      |
| 2             | 4096        | 8192          | 0.1563        | 0.0051        | 0.0533     | 1.270      |
| 1             | 8192        | 8192          | 0.1841        | 0.0051        | 0.0446     | 1.269      |

### Table 2 – Matrix with *Varying* Total Threads (Threads/Block = 1024)

| Threads/Block | Blocks/Grid | Total Threads | GPU Time (ms) | CPU Time (ms) | GPU GFLOPS | CPU GFLOPS |
| :-----------: | :---------: | :-----------: | :-----------: | :-----------: | :--------: | :--------: |
| 1024          | 8           | 8192          | 0.1231        | 0.0050        | 0.0703     | 1.270      |
| 1024          | 4           | 4096          | 0.1205        | 0.0051        | 0.0715     | 1.269      |
| 1024          | 2           | 2048          | 0.1250        | 0.0051        | 0.0714     | 1.268      |
| 1024          | 1           | 1024          | 0.1241        | 0.0051        | 0.0702     | 1.271      |

## Discussion

### Performance Analysis

For the  matrix (6400x6400, requiring only 6400 additions), the sequential CPU baseline is significantly faster than the GPU implementation across all configurations tested. The CPU time is consistently around 0.005 ms, while the GPU time (including transfers) is around 0.12 - 0.18 ms. This results in a GPU speedup with data transfer of only around 0.03x to 0.04x, meaning the GPU takes roughly 20 times longer than the CPU. Even the kernel-only speedup is very low, around 0.06x.

The primary reason for the GPU's poor performance on this  workload is **overhead**. The problem size (6400 floating-point additions) is computationally trivial for modern hardware. The time taken for kernel launch (~10 µs, not directly measured but implied overhead) and, more significantly, the PCIe data transfers between the CPU and GPU dominate the execution time. This task is **bandwidth-starved**, where the cost of moving the data to the GPU is much higher than the cost of computing on it. The working set fits comfortably within the CPU's cache hierarchy, allowing the CPU to perform the calculation very quickly without needing high bandwidth to external memory.

Varying the thread and block configuration on the  matrix has limited impact on the overall GPU runtime, as the transfer overhead remains the bottleneck. While GPU GFLOPS show a slight peak for block sizes between 128 and 256 threads when keeping the total thread count fixed (Table 1), this minor improvement in kernel throughput is insufficient to overcome the data transfer and launch latencies and does not translate to beating the CPU. Very  blocks (e.g., <= 8 threads) hurt performance due to increased scheduling and merge overhead, while very large blocks (e.g., 1024 threads) lead to low SM occupancy.


## Recommended Configurations

Based on the performance analysis:

*   **(6400×6400) Matrix**: **Run on CPU**. The CPU is significantly faster due to low arithmetic intensity and GPU overhead. If GPU execution is required (e.g., as part of a larger batched workload), configurations around **128–256 threads per block** and **2⁶–2⁷ blocks** achieve the highest GPU GFLOPS, although still much slower than the CPU.
